# Science Fair Project: Synthetic Chest X-Ray Generation with Diffusion Models
## Improving Pneumonia Classifier Accuracy Through Synthetic Data Augmentation

**Author:** David  
**Date:** January 2026  
**Hardware:** 
- Google Colab Pro (A100 80GB) â€” Diffusion model training & image generation
- Local PC (RTX 5070TI 16GB + 32GB RAM) â€” Classifier training & experiments

**Dataset:** [hf-vision/chest-xray-pneumonia](https://huggingface.co/datasets/hf-vision/chest-xray-pneumonia)

---

## Table of Contents
1. [Project Overview](#1-project-overview)
2. [Scientific Background](#2-scientific-background)
3. [Hardware Strategy](#3-hardware-strategy)
4. [Choosing Your Diffusion Model](#4-choosing-your-diffusion-model)
5. [Finetuning Techniques Comparison](#5-finetuning-techniques-comparison)
6. [Dataset Preparation](#6-dataset-preparation)
7. [Training the Diffusion Model](#7-training-the-diffusion-model)
8. [Generating Synthetic Images](#8-generating-synthetic-images)
9. [Evaluating Synthetic Image Quality](#9-evaluating-synthetic-image-quality)
10. [Training the Image Classifier (Local PC)](#10-training-the-image-classifier-local-pc)
11. [Experimental Design](#11-experimental-design)
12. [Complete Code Examples](#12-complete-code-examples)
13. [Expected Results & Presentation Tips](#13-expected-results--presentation-tips)
14. [Resources & References](#14-resources--references)

---

## 1. Project Overview

### Research Question
**Can synthetic chest X-ray images generated by a finetuned diffusion model improve the accuracy of a pneumonia image classifier?**

### Hypothesis
Training a pneumonia classifier on a combination of real and synthetic chest X-ray images will yield higher accuracy than training on real images alone, particularly when the original dataset is limited.

### Project Pipeline
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PROJECT PIPELINE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    COLAB PRO (A100 80GB)                            â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  1. Dataset Prep    2. Full DreamBooth       3. Generate Synthetic  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ Chest X-rayâ”‚ â”€â”€â–º â”‚ SD 2.1 + Text   â”‚ â”€â”€â–º  â”‚ 1000+ synthetic â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Dataset    â”‚      â”‚ Encoder Trainingâ”‚      â”‚ images/class    â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼ Download synthetic images              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 LOCAL PC (RTX 5070TI 16GB)                          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  4. Quality Eval    5. Classifier Training    6. Results Analysis   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ FID Score â”‚      â”‚ 7 Experiments   â”‚      â”‚ Statistical     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ MS-SSIM   â”‚      â”‚ DenseNet-121    â”‚      â”‚ Comparison      â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Scientific Background

### Why Synthetic Medical Images Matter

Medical imaging datasets face unique challenges:
- **Data scarcity**: Collecting and annotating medical images is expensive and time-consuming
- **Privacy concerns**: Patient data is protected by regulations like HIPAA
- **Class imbalance**: Rare diseases have very few examples
- **Expert annotation required**: Only trained radiologists can accurately label images

Diffusion models offer a solution by generating realistic synthetic images that can augment training data without compromising patient privacy.

### How Diffusion Models Work

Diffusion models learn to generate images through a two-step process:

1. **Forward Process (Adding Noise)**: Gradually add Gaussian noise to images until they become pure noise
2. **Reverse Process (Denoising)**: Learn to reverse this process, recovering the original image from noise

```
Forward:  Clean Image â†’ Slightly Noisy â†’ Very Noisy â†’ Pure Noise
Reverse:  Pure Noise â†’ Slightly Less Noisy â†’ ... â†’ Generated Image
```

### Latent Diffusion Models (Stable Diffusion)

Instead of working directly with pixels, Stable Diffusion:
1. Encodes images into a compressed latent space using a VAE (Variational Autoencoder)
2. Performs diffusion in this smaller latent space (much faster!)
3. Decodes the result back to pixel space

This makes training and generation significantly more efficient.

### Key Prior Research

| Paper | Key Finding |
|-------|-------------|
| RoentGen (Stanford, 2023) | Fine-tuned SD generates realistic CXRs controlled by text prompts |
| Medfusion (2023) | LDMs achieve FID of 17.28 vs 28.69 for StyleGAN-3 on chest X-rays |
| DreamBooth Medical (2024) | DreamBooth successfully generates high-quality medical images with few training samples |
| Diff-CXR (2024) | 33.4% improvement in FID score over previous methods |

---

## 3. Hardware Strategy

### Why Split the Workload?

| Task | Compute Intensity | Best Hardware |
|------|-------------------|---------------|
| Diffusion model finetuning | ğŸ”´ Very High (40-80GB VRAM) | Colab A100 80GB |
| Synthetic image generation | ğŸŸ  High (8-16GB VRAM) | Colab A100 80GB |
| FID/quality evaluation | ğŸŸ¡ Medium (4-8GB VRAM) | Either |
| Classifier training | ğŸŸ¢ Low (4-6GB VRAM) | Local RTX 5070TI |
| Experiments & analysis | ğŸŸ¢ Low | Local PC |

### Benefits of This Split

1. **Save Colab compute units** â€” Classifier training is cheap, don't waste A100 time on it
2. **Faster iteration** â€” Run classifier experiments locally without internet dependency
3. **Parallel work** â€” Generate more synthetic images on Colab while training locally
4. **No timeouts** â€” Local training won't disconnect after 90 minutes of "inactivity"

### Your Hardware Capabilities

#### Colab Pro A100 80GB
```
âœ… Full DreamBooth finetuning (not just LoRA)
âœ… Train text encoder (better medical terminology)
âœ… Batch size 8-16 for training
âœ… Generate thousands of synthetic images
âœ… Run quality metrics (FID) on large sets
```

#### Local PC (RTX 5070TI 16GB + 32GB RAM)
```
âœ… DenseNet-121 classifier with batch size 64-128
âœ… ResNet-50 classifier with batch size 32-64
âœ… Vision Transformer (ViT) with batch size 16-32
âœ… All 7 experiments in a few hours
âœ… Full statistical analysis
âœ… Visualization generation
```

---

## 4. Choosing Your Diffusion Model

### Model Comparison

| Model | Resolution | VRAM Needed | Training Time | Recommendation |
|-------|-----------|-------------|---------------|----------------|
| **Stable Diffusion 1.5** | 512Ã—512 | ~12GB | Fast | Good but older |
| **Stable Diffusion 2.1** | 512Ã—512 | ~16-24GB | Medium | â­â­ **Best choice** |
| **SDXL** | 1024Ã—1024 | ~40GB+ | Slow | Overkill for CXR |
| **SD 3.0/3.5** | Various | ~50GB+ | Very Slow | Too new |

### ğŸ† Recommended: Stable Diffusion 2.1 Base

**Why SD 2.1 for your project:**
- âœ… 512Ã—512 resolution matches common CXR preprocessing
- âœ… Well within your 80GB VRAM budget
- âœ… Proven in medical imaging research (RoentGen, Medfusion)
- âœ… Can train text encoder for better medical terminology
- âœ… OpenRAIL++ license allows research use

**Model ID:** `stabilityai/stable-diffusion-2-1-base`

### Why NOT SDXL?

Even with 80GB VRAM, SDXL isn't recommended because:
1. **Resolution overkill**: Classifiers resize to 224Ã—224 anyway
2. **Training time**: 2-3x longer than SD 2.1
3. **Research validation**: Published papers use SD 1.5/2.1
4. **Diminishing returns**: Higher resolution doesn't improve diagnostic features

---

## 5. Finetuning Techniques Comparison

### Overview of Methods

| Method | What it Does | VRAM Needed | Quality |
|--------|--------------|-------------|---------|
| **Full DreamBooth** | Updates all model weights | ~24-40GB | â­â­â­ Best |
| **DreamBooth + LoRA** | Low-rank adaptation | ~12-16GB | â­â­ Great |
| **LoRA only** | Trains small adapter | ~8-12GB | â­ Good |

### ğŸ† Recommended: Full DreamBooth with Text Encoder

**With your 80GB A100, you can use the highest quality approach!**

Full DreamBooth advantages:
- âœ… Best image quality and realism
- âœ… Better understanding of medical terminology (text encoder training)
- âœ… No quality compromise from low-rank approximation
- âœ… You have the VRAMâ€”use it!

### Training Configuration for A100 80GB

```python
# Optimal settings for A100 80GB - Full DreamBooth
training_config = {
    "method": "full_dreambooth",
    "resolution": 512,
    "train_batch_size": 8,
    "gradient_accumulation_steps": 2,  # Effective batch size: 16
    "learning_rate": 5e-6,  # Lower for full finetuning
    "max_train_steps": 800,
    "train_text_encoder": True,  # ENABLED with 80GB!
    "with_prior_preservation": True,
    "prior_loss_weight": 1.0,
    "num_class_images": 200,
    "mixed_precision": "fp16",
    "gradient_checkpointing": False,  # Not needed with 80GB
}
```

### Alternative: LoRA with Maximum Settings

If you want faster training and smaller output files:

```python
# LoRA settings for A100 80GB
lora_config = {
    "method": "dreambooth_lora",
    "resolution": 512,
    "train_batch_size": 16,  # Can go higher with LoRA
    "learning_rate": 1e-4,
    "max_train_steps": 1500,
    "train_text_encoder": True,
    "rank": 128,  # Higher rank = more capacity
    "lora_alpha": 128,
    "mixed_precision": "fp16",
}
```

---

## 6. Dataset Preparation

### Dataset Overview

The **hf-vision/chest-xray-pneumonia** dataset contains:
- **Total images**: 5,863 JPEG X-rays
- **Classes**: Normal (1,583) | Pneumonia (4,273)
- **Split**: Train / Val / Test folders
- **Source**: Pediatric patients (1-5 years) from Guangzhou hospital
- **License**: CC-BY-4.0

### Loading the Dataset

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("hf-vision/chest-xray-pneumonia")

# Check structure
print(dataset)
# DatasetDict({
#     'train': Dataset({features: ['image', 'label'], num_rows: 5216}),
#     'validation': Dataset({features: ['image', 'label'], num_rows: 16}),
#     'test': Dataset({features: ['image', 'label'], num_rows: 624})
# })

# Labels: 0 = NORMAL, 1 = PNEUMONIA
```

### Preprocessing for Diffusion Model Training

```python
from torchvision import transforms
from PIL import Image
import os
from tqdm import tqdm

def prepare_training_data(dataset, output_dir, target_size=512):
    """
    Prepare images for DreamBooth training.
    Saves images to separate folders by class.
    """
    os.makedirs(f"{output_dir}/normal", exist_ok=True)
    os.makedirs(f"{output_dir}/pneumonia", exist_ok=True)
    
    transform = transforms.Compose([
        transforms.Resize((target_size, target_size)),
    ])
    
    for idx, sample in enumerate(tqdm(dataset['train'], desc="Preparing data")):
        img = sample['image']
        label = sample['label']
        
        # Convert to RGB if grayscale
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Transform
        img = transform(img)
        
        # Save
        class_name = "normal" if label == 0 else "pneumonia"
        img.save(f"{output_dir}/{class_name}/{idx:05d}.png")
    
    print(f"Saved images to {output_dir}")
    return output_dir

# Prepare data
prepare_training_data(dataset, "./cxr_training_data")
```

### Creating Class Images for Prior Preservation

DreamBooth uses prior preservation to maintain model diversity:

```python
from diffusers import StableDiffusionPipeline
import torch

def generate_class_images(num_images=200, output_dir="./class_images"):
    """Generate class images for prior preservation."""
    
    pipe = StableDiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-1-base",
        torch_dtype=torch.float16,
        safety_checker=None
    ).to("cuda")
    
    os.makedirs(output_dir, exist_ok=True)
    
    prompt = "a chest x-ray image, medical radiograph"
    
    for i in tqdm(range(num_images), desc="Generating class images"):
        image = pipe(
            prompt,
            num_inference_steps=30,
            guidance_scale=7.5,
            generator=torch.Generator("cuda").manual_seed(i)
        ).images[0]
        image.save(f"{output_dir}/{i:04d}.png")
    
    del pipe
    torch.cuda.empty_cache()

generate_class_images(200, "./class_images/xray")
```

---

## 7. Training the Diffusion Model

### Installation & Setup (Colab)

```python
# ================================================
# CELL 1: Environment Setup
# ================================================
!pip install -q diffusers transformers accelerate bitsandbytes
!pip install -q peft xformers
!pip install -q datasets huggingface_hub

import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Clone diffusers for training scripts
!git clone https://github.com/huggingface/diffusers
%cd diffusers
!pip install -e .
%cd examples/dreambooth
!pip install -r requirements.txt

# Initialize accelerate
!accelerate config default
```

### Full DreamBooth Training (Recommended for 80GB)

```bash
# Train for NORMAL class - Full DreamBooth with Text Encoder
export MODEL_NAME="stabilityai/stable-diffusion-2-1-base"
export INSTANCE_DIR="./cxr_training_data/normal"
export CLASS_DIR="./class_images/xray"
export OUTPUT_DIR="./models/cxr_full_normal"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a chest xray of healthy normal lungs, clear lung fields" \
  --class_prompt="a chest xray image" \
  --resolution=512 \
  --train_batch_size=8 \
  --gradient_accumulation_steps=2 \
  --learning_rate=5e-6 \
  --lr_scheduler="cosine" \
  --lr_warmup_steps=100 \
  --max_train_steps=800 \
  --train_text_encoder \
  --with_prior_preservation \
  --prior_loss_weight=1.0 \
  --num_class_images=200 \
  --mixed_precision="fp16" \
  --validation_prompt="a chest xray of healthy normal lungs" \
  --num_validation_images=4 \
  --validation_steps=200 \
  --seed=42
```

```bash
# Train for PNEUMONIA class - Full DreamBooth with Text Encoder
export MODEL_NAME="stabilityai/stable-diffusion-2-1-base"
export INSTANCE_DIR="./cxr_training_data/pneumonia"
export CLASS_DIR="./class_images/xray"
export OUTPUT_DIR="./models/cxr_full_pneumonia"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a chest xray showing pneumonia infection, lung opacity" \
  --class_prompt="a chest xray image" \
  --resolution=512 \
  --train_batch_size=8 \
  --gradient_accumulation_steps=2 \
  --learning_rate=5e-6 \
  --lr_scheduler="cosine" \
  --lr_warmup_steps=100 \
  --max_train_steps=800 \
  --train_text_encoder \
  --with_prior_preservation \
  --prior_loss_weight=1.0 \
  --num_class_images=200 \
  --mixed_precision="fp16" \
  --validation_prompt="a chest xray showing pneumonia" \
  --num_validation_images=4 \
  --validation_steps=200 \
  --seed=42
```

### Alternative: LoRA Training (Faster, Smaller Files)

```bash
# LoRA training for NORMAL class
accelerate launch train_dreambooth_lora.py \
  --pretrained_model_name_or_path="stabilityai/stable-diffusion-2-1-base" \
  --instance_data_dir="./cxr_training_data/normal" \
  --output_dir="./models/cxr_lora_normal" \
  --instance_prompt="a chest xray of healthy normal lungs" \
  --resolution=512 \
  --train_batch_size=16 \
  --learning_rate=1e-4 \
  --max_train_steps=1500 \
  --train_text_encoder \
  --rank=128 \
  --mixed_precision="fp16" \
  --seed=42
```

### Training Time Estimates (A100 80GB)

| Method | Per Class | Total (Both Classes) |
|--------|-----------|---------------------|
| Full DreamBooth | ~20-25 min | ~45-50 min |
| LoRA (rank 128) | ~12-15 min | ~25-30 min |

### Monitoring Training

Key metrics to watch:
- **Loss**: Should decrease steadily, target < 0.1
- **Validation images**: Check every 200 steps
- **No mode collapse**: Ensure diverse outputs

---

## 8. Generating Synthetic Images

### Loading the Trained Model

```python
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
import torch
import os
from tqdm import tqdm

def load_finetuned_model(model_path):
    """Load fully finetuned DreamBooth model."""
    
    pipe = StableDiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        safety_checker=None,
    ).to("cuda")
    
    # Use faster scheduler for generation
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
    
    return pipe

# Load both models
pipe_normal = load_finetuned_model("./models/cxr_full_normal")
pipe_pneumonia = load_finetuned_model("./models/cxr_full_pneumonia")
```

### Generating Synthetic Dataset

```python
def generate_synthetic_dataset(
    pipe,
    prompt: str,
    num_images: int,
    output_dir: str,
    batch_size: int = 8,  # Larger batch with 80GB
    num_inference_steps: int = 30,
    guidance_scale: float = 7.5,
):
    """Generate synthetic chest X-ray images."""
    
    os.makedirs(output_dir, exist_ok=True)
    
    generated = 0
    pbar = tqdm(total=num_images, desc="Generating images")
    
    while generated < num_images:
        current_batch = min(batch_size, num_images - generated)
        
        images = pipe(
            [prompt] * current_batch,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=torch.Generator("cuda").manual_seed(generated),
        ).images
        
        for i, img in enumerate(images):
            img.save(f"{output_dir}/synthetic_{generated + i:05d}.png")
        
        generated += current_batch
        pbar.update(current_batch)
    
    pbar.close()
    return output_dir

# Generate 1000+ synthetic images per class
generate_synthetic_dataset(
    pipe_normal,
    prompt="a high quality chest xray of healthy normal lungs, clear lung fields, medical imaging",
    num_images=1200,
    output_dir="./synthetic_data/normal"
)

generate_synthetic_dataset(
    pipe_pneumonia,
    prompt="a high quality chest xray showing pneumonia infection, lung opacity visible, medical imaging",
    num_images=1200,
    output_dir="./synthetic_data/pneumonia"
)
```

### Download Synthetic Images to Local PC

```python
# On Colab: Zip the synthetic data
!zip -r synthetic_data.zip ./synthetic_data

# Download via Colab files
from google.colab import files
files.download('synthetic_data.zip')

# OR mount Google Drive and copy
from google.colab import drive
drive.mount('/content/drive')
!cp -r ./synthetic_data /content/drive/MyDrive/science_fair/
```

---

## 9. Evaluating Synthetic Image Quality

### Key Metrics

| Metric | What it Measures | Good Score |
|--------|------------------|------------|
| **FID** (FrÃ©chet Inception Distance) | Distribution similarity | Lower is better (< 50 good, < 20 excellent) |
| **MS-SSIM** | Diversity (inverse) | Lower avg = more diverse |
| **Inception Score** | Quality + diversity | Higher is better |

### Computing FID Score

```python
# Can run on Colab or locally (needs ~4GB VRAM)
!pip install pytorch-fid

from pytorch_fid import fid_score
import torch

def calculate_fid(real_path: str, synthetic_path: str, batch_size: int = 50):
    """Calculate FID between real and synthetic image sets."""
    
    fid_value = fid_score.calculate_fid_given_paths(
        [real_path, synthetic_path],
        batch_size=batch_size,
        device=torch.device('cuda'),
        dims=2048,
        num_workers=4,
    )
    
    return fid_value

# Calculate FID for each class
fid_normal = calculate_fid(
    "./cxr_training_data/normal",
    "./synthetic_data/normal"
)
print(f"FID (Normal): {fid_normal:.2f}")

fid_pneumonia = calculate_fid(
    "./cxr_training_data/pneumonia", 
    "./synthetic_data/pneumonia"
)
print(f"FID (Pneumonia): {fid_pneumonia:.2f}")
```

### Visual Comparison Grid

```python
import matplotlib.pyplot as plt
from PIL import Image
import random
import os

def create_comparison_grid(real_dir, synth_dir, num_samples=5, save_path="comparison.png"):
    """Create side-by-side comparison grid."""
    
    fig, axes = plt.subplots(2, num_samples, figsize=(3*num_samples, 6))
    
    real_images = random.sample(os.listdir(real_dir), num_samples)
    synth_images = random.sample(os.listdir(synth_dir), num_samples)
    
    for i in range(num_samples):
        # Real images (top row)
        img = Image.open(f"{real_dir}/{real_images[i]}")
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].axis('off')
        if i == num_samples // 2:
            axes[0, i].set_title('Real Images', fontsize=14, fontweight='bold')
        
        # Synthetic images (bottom row)
        img = Image.open(f"{synth_dir}/{synth_images[i]}")
        axes[1, i].imshow(img, cmap='gray')
        axes[1, i].axis('off')
        if i == num_samples // 2:
            axes[1, i].set_title('Synthetic Images', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"Saved to {save_path}")

create_comparison_grid(
    "./cxr_training_data/normal", 
    "./synthetic_data/normal",
    save_path="comparison_normal.png"
)
```

---

## 10. Training the Image Classifier (Local PC)

### Why Train Locally?

Your RTX 5070TI (16GB VRAM) is **perfect** for classifier training:

| Model | Batch Size Possible | Training Time (20 epochs) |
|-------|--------------------|-----------------------------|
| DenseNet-121 | 64-128 | ~15-20 min |
| ResNet-50 | 32-64 | ~20-25 min |
| EfficientNet-B0 | 64-128 | ~12-15 min |
| ViT-Base | 16-32 | ~25-30 min |

### Local Environment Setup

```bash
# Create conda environment
conda create -n science_fair python=3.10
conda activate science_fair

# Install PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install other dependencies
pip install scikit-learn pandas matplotlib seaborn tqdm
pip install pytorch-fid  # For FID calculation
```

### Dataset Class

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os

class ChestXrayDataset(Dataset):
    """Custom dataset for chest X-rays with synthetic data support."""
    
    def __init__(
        self, 
        real_dirs: dict = None,
        synthetic_dirs: dict = None,
        real_ratio: float = 1.0,
        synthetic_ratio: float = 0.0,
        transform=None,
        seed: int = 42
    ):
        """
        Args:
            real_dirs: {'normal': 'path/to/normal', 'pneumonia': 'path/to/pneumonia'}
            synthetic_dirs: Same structure for synthetic images
            real_ratio: Fraction of real images to use (0.0 to 1.0)
            synthetic_ratio: Fraction of synthetic images to use
            transform: Image transformations
            seed: Random seed for reproducible subsampling
        """
        self.samples = []
        self.transform = transform
        
        torch.manual_seed(seed)
        
        # Add real images
        if real_dirs and real_ratio > 0:
            for label_name, dir_path in real_dirs.items():
                label = 0 if label_name == 'normal' else 1
                files = [f for f in os.listdir(dir_path) if f.endswith(('.png', '.jpg', '.jpeg'))]
                
                # Subsample
                num_samples = int(len(files) * real_ratio)
                selected = torch.randperm(len(files))[:num_samples].tolist()
                
                for idx in selected:
                    self.samples.append((os.path.join(dir_path, files[idx]), label, 'real'))
        
        # Add synthetic images
        if synthetic_dirs and synthetic_ratio > 0:
            for label_name, dir_path in synthetic_dirs.items():
                label = 0 if label_name == 'normal' else 1
                files = [f for f in os.listdir(dir_path) if f.endswith(('.png', '.jpg', '.jpeg'))]
                
                # Subsample
                num_samples = int(len(files) * synthetic_ratio)
                selected = torch.randperm(len(files))[:num_samples].tolist()
                
                for idx in selected:
                    self.samples.append((os.path.join(dir_path, files[idx]), label, 'synthetic'))
        
        print(f"Dataset: {len(self.samples)} images "
              f"({sum(1 for s in self.samples if s[2]=='real')} real, "
              f"{sum(1 for s in self.samples if s[2]=='synthetic')} synthetic)")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label, source = self.samples[idx]
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, label


def get_transforms(train=True):
    """Get image transforms."""
    if train:
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    else:
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
```

### Classifier Model

```python
import torch.nn as nn
import torchvision.models as models

class PneumoniaClassifier(nn.Module):
    """DenseNet-121 based classifier for pneumonia detection."""
    
    def __init__(self, num_classes=2, pretrained=True):
        super().__init__()
        
        # Load pretrained DenseNet-121
        weights = models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None
        self.densenet = models.densenet121(weights=weights)
        
        # Replace classifier head
        num_features = self.densenet.classifier.in_features
        self.densenet.classifier = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        return self.densenet(x)


def count_parameters(model):
    """Count trainable parameters."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# Test
model = PneumoniaClassifier()
print(f"Trainable parameters: {count_parameters(model):,}")
```

### Training Function

```python
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from tqdm import tqdm
import numpy as np

def train_classifier(
    model,
    train_loader,
    val_loader,
    num_epochs=20,
    learning_rate=1e-4,
    device='cuda',
    save_path='best_model.pth'
):
    """Train the pneumonia classifier."""
    
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    best_val_acc = 0
    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []}
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        
        for images, labels in train_pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        train_loss /= len(train_loader)
        
        # Validation phase
        model.eval()
        val_loss = 0
        all_preds, all_labels, all_probs = [], [], []
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]"):
                images, labels = images.to(device), labels.to(device)
                
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs[:, 1].cpu().numpy())
        
        val_loss /= len(val_loader)
        
        # Calculate metrics
        val_acc = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')
        auc = roc_auc_score(all_labels, all_probs)
        
        # Log
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(f1)
        history['val_auc'].append(auc)
        
        print(f"\nEpoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, "
              f"Acc={val_acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}")
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), save_path)
            print(f"  âœ“ Saved best model (acc={val_acc:.4f})")
        
        scheduler.step()
    
    return history, best_val_acc
```

### Training Configuration for RTX 5070TI

```python
# Optimal settings for RTX 5070TI (16GB VRAM)
LOCAL_CONFIG = {
    "batch_size": 64,  # Can go up to 128 with DenseNet-121
    "num_workers": 4,
    "learning_rate": 1e-4,
    "num_epochs": 20,
    "device": "cuda",
}

# Create dataloaders
train_dataset = ChestXrayDataset(
    real_dirs={'normal': './data/real/normal', 'pneumonia': './data/real/pneumonia'},
    synthetic_dirs={'normal': './data/synthetic/normal', 'pneumonia': './data/synthetic/pneumonia'},
    real_ratio=1.0,
    synthetic_ratio=0.5,  # Adjust per experiment
    transform=get_transforms(train=True)
)

train_loader = DataLoader(
    train_dataset,
    batch_size=LOCAL_CONFIG["batch_size"],
    shuffle=True,
    num_workers=LOCAL_CONFIG["num_workers"],
    pin_memory=True
)
```

---

## 11. Experimental Design

### Experiment Matrix

| Experiment | Real Images | Synthetic Images | Total | Purpose |
|------------|-------------|------------------|-------|---------|
| **Baseline** | 100% (5,216) | 0 | 5,216 | Control group |
| **Synth-25** | 100% (5,216) | 25% (1,304) | 6,520 | Small augmentation |
| **Synth-50** | 100% (5,216) | 50% (2,608) | 7,824 | Medium augmentation |
| **Synth-100** | 100% (5,216) | 100% (5,216) | 10,432 | Full augmentation |
| **Limited-10** | 10% (522) | 100% (5,216) | 5,738 | Severe data scarcity |
| **Limited-25** | 25% (1,304) | 100% (5,216) | 6,520 | Moderate data scarcity |
| **Synth-Only** | 0 | 100% (5,216) | 5,216 | Pure synthetic baseline |

### Running All Experiments

```python
import json
from datetime import datetime

def run_all_experiments(
    real_dirs: dict,
    synthetic_dirs: dict,
    test_dirs: dict,
    output_dir: str = "./experiment_results"
):
    """Run all experiments and save results."""
    
    os.makedirs(output_dir, exist_ok=True)
    
    experiments = {
        'baseline':    {'real_ratio': 1.0,  'synthetic_ratio': 0.0},
        'synth_25':    {'real_ratio': 1.0,  'synthetic_ratio': 0.25},
        'synth_50':    {'real_ratio': 1.0,  'synthetic_ratio': 0.50},
        'synth_100':   {'real_ratio': 1.0,  'synthetic_ratio': 1.0},
        'limited_10':  {'real_ratio': 0.10, 'synthetic_ratio': 1.0},
        'limited_25':  {'real_ratio': 0.25, 'synthetic_ratio': 1.0},
        'synth_only':  {'real_ratio': 0.0,  'synthetic_ratio': 1.0},
    }
    
    # Test set (same for all experiments - IMPORTANT!)
    test_dataset = ChestXrayDataset(
        real_dirs=test_dirs,
        transform=get_transforms(train=False)
    )
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)
    
    all_results = {}
    
    for exp_name, config in experiments.items():
        print(f"\n{'='*60}")
        print(f"EXPERIMENT: {exp_name}")
        print(f"{'='*60}")
        
        # Run 3 times with different seeds for statistical significance
        exp_accuracies = []
        exp_f1s = []
        exp_aucs = []
        
        for seed in [42, 123, 456]:
            print(f"\n--- Seed {seed} ---")
            
            # Create training dataset
            train_dataset = ChestXrayDataset(
                real_dirs=real_dirs,
                synthetic_dirs=synthetic_dirs,
                real_ratio=config['real_ratio'],
                synthetic_ratio=config['synthetic_ratio'],
                transform=get_transforms(train=True),
                seed=seed
            )
            
            train_loader = DataLoader(
                train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True
            )
            
            # Train model
            model = PneumoniaClassifier(pretrained=True)
            torch.manual_seed(seed)
            
            history, best_acc = train_classifier(
                model, train_loader, test_loader,
                num_epochs=20,
                save_path=f"{output_dir}/{exp_name}_seed{seed}.pth"
            )
            
            exp_accuracies.append(best_acc)
            exp_f1s.append(max(history['val_f1']))
            exp_aucs.append(max(history['val_auc']))
        
        # Store results
        all_results[exp_name] = {
            'config': config,
            'accuracy_mean': np.mean(exp_accuracies),
            'accuracy_std': np.std(exp_accuracies),
            'f1_mean': np.mean(exp_f1s),
            'f1_std': np.std(exp_f1s),
            'auc_mean': np.mean(exp_aucs),
            'auc_std': np.std(exp_aucs),
            'all_accuracies': exp_accuracies,
        }
        
        print(f"\n{exp_name} Results: Acc={np.mean(exp_accuracies):.4f}Â±{np.std(exp_accuracies):.4f}")
    
    # Save results
    with open(f"{output_dir}/results.json", 'w') as f:
        json.dump(all_results, f, indent=2)
    
    return all_results

# Run experiments
results = run_all_experiments(
    real_dirs={'normal': './data/real/normal', 'pneumonia': './data/real/pneumonia'},
    synthetic_dirs={'normal': './data/synthetic/normal', 'pneumonia': './data/synthetic/pneumonia'},
    test_dirs={'normal': './data/test/normal', 'pneumonia': './data/test/pneumonia'},
)
```

### Statistical Analysis

```python
from scipy import stats

def analyze_and_print_results(results: dict):
    """Analyze results with statistical tests."""
    
    baseline_accs = results['baseline']['all_accuracies']
    
    print("\n" + "="*80)
    print("FINAL RESULTS SUMMARY")
    print("="*80)
    print(f"{'Experiment':<15} {'Accuracy':<18} {'F1 Score':<18} {'AUC':<18} {'vs Baseline':<15}")
    print("-"*80)
    
    for exp_name, data in results.items():
        acc_str = f"{data['accuracy_mean']:.4f} Â± {data['accuracy_std']:.4f}"
        f1_str = f"{data['f1_mean']:.4f} Â± {data['f1_std']:.4f}"
        auc_str = f"{data['auc_mean']:.4f} Â± {data['auc_std']:.4f}"
        
        if exp_name == 'baseline':
            delta_str = "(baseline)"
        else:
            delta = data['accuracy_mean'] - results['baseline']['accuracy_mean']
            _, p_value = stats.ttest_ind(baseline_accs, data['all_accuracies'])
            sig = "*" if p_value < 0.05 else ""
            delta_str = f"{delta:+.4f} (p={p_value:.3f}){sig}"
        
        print(f"{exp_name:<15} {acc_str:<18} {f1_str:<18} {auc_str:<18} {delta_str:<15}")
    
    print("-"*80)
    print("* = statistically significant (p < 0.05)")

analyze_and_print_results(results)
```

---

## 12. Complete Code Examples

### Complete Colab Notebook (Diffusion Training)

```python
# ============================================================
# COLAB NOTEBOOK: Chest X-Ray Diffusion Model Training
# Hardware: A100 80GB
# ============================================================

# CELL 1: Setup
!pip install -q diffusers transformers accelerate peft
!pip install -q datasets huggingface_hub pytorch-fid

import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# CELL 2: Download and prepare dataset
from datasets import load_dataset
import os
from tqdm import tqdm
from PIL import Image
from torchvision import transforms

dataset = load_dataset("hf-vision/chest-xray-pneumonia")

os.makedirs("./data/normal", exist_ok=True)
os.makedirs("./data/pneumonia", exist_ok=True)

transform = transforms.Resize((512, 512))

for idx, sample in enumerate(tqdm(dataset['train'])):
    img = sample['image'].convert('RGB')
    img = transform(img)
    label = 'normal' if sample['label'] == 0 else 'pneumonia'
    img.save(f"./data/{label}/{idx:05d}.png")

# CELL 3: Clone diffusers and install
!git clone https://github.com/huggingface/diffusers
%cd diffusers
!pip install -e .
%cd examples/dreambooth
!pip install -r requirements.txt
!accelerate config default

# CELL 4: Train Normal class (Full DreamBooth)
!accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path="stabilityai/stable-diffusion-2-1-base" \
  --instance_data_dir="../../data/normal" \
  --output_dir="../../models/cxr_normal" \
  --instance_prompt="chest xray healthy normal lungs clear" \
  --resolution=512 \
  --train_batch_size=8 \
  --gradient_accumulation_steps=2 \
  --learning_rate=5e-6 \
  --max_train_steps=800 \
  --train_text_encoder \
  --mixed_precision="fp16"

# CELL 5: Train Pneumonia class
!accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path="stabilityai/stable-diffusion-2-1-base" \
  --instance_data_dir="../../data/pneumonia" \
  --output_dir="../../models/cxr_pneumonia" \
  --instance_prompt="chest xray pneumonia infection opacity" \
  --resolution=512 \
  --train_batch_size=8 \
  --gradient_accumulation_steps=2 \
  --learning_rate=5e-6 \
  --max_train_steps=800 \
  --train_text_encoder \
  --mixed_precision="fp16"

# CELL 6: Generate synthetic images
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler

for class_name, prompt in [
    ("normal", "high quality chest xray healthy normal lungs"),
    ("pneumonia", "high quality chest xray pneumonia infection")
]:
    pipe = StableDiffusionPipeline.from_pretrained(
        f"../../models/cxr_{class_name}",
        torch_dtype=torch.float16,
        safety_checker=None
    ).to("cuda")
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
    
    os.makedirs(f"./synthetic/{class_name}", exist_ok=True)
    
    for i in tqdm(range(1200)):
        img = pipe(prompt, num_inference_steps=30).images[0]
        img.save(f"./synthetic/{class_name}/{i:05d}.png")
    
    del pipe
    torch.cuda.empty_cache()

# CELL 7: Calculate FID
from pytorch_fid import fid_score

for class_name in ["normal", "pneumonia"]:
    fid = fid_score.calculate_fid_given_paths(
        [f"../../data/{class_name}", f"./synthetic/{class_name}"],
        batch_size=50, device=torch.device('cuda'), dims=2048
    )
    print(f"FID ({class_name}): {fid:.2f}")

# CELL 8: Zip and download
!zip -r synthetic_data.zip ./synthetic
from google.colab import files
files.download('synthetic_data.zip')
```

### Complete Local Script (Classifier Training)

```python
# ============================================================
# LOCAL SCRIPT: Classifier Training & Experiments
# Hardware: RTX 5070TI 16GB
# Save as: train_classifier.py
# ============================================================

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torchvision.models as models
from torchvision import transforms
from PIL import Image
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from scipy import stats
from tqdm import tqdm
import numpy as np
import os
import json
from datetime import datetime

# [Include ChestXrayDataset, PneumoniaClassifier, get_transforms, train_classifier from above]

def main():
    # Paths - UPDATE THESE
    DATA_DIR = "./data"
    RESULTS_DIR = "./results"
    
    real_dirs = {
        'normal': f"{DATA_DIR}/real/normal",
        'pneumonia': f"{DATA_DIR}/real/pneumonia"
    }
    synthetic_dirs = {
        'normal': f"{DATA_DIR}/synthetic/normal",
        'pneumonia': f"{DATA_DIR}/synthetic/pneumonia"
    }
    test_dirs = {
        'normal': f"{DATA_DIR}/test/normal",
        'pneumonia': f"{DATA_DIR}/test/pneumonia"
    }
    
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # Run all experiments
    results = run_all_experiments(real_dirs, synthetic_dirs, test_dirs, RESULTS_DIR)
    
    # Print summary
    analyze_and_print_results(results)
    
    print(f"\nResults saved to {RESULTS_DIR}/results.json")

if __name__ == "__main__":
    main()
```

---

## 13. Expected Results & Presentation Tips

### Anticipated Findings

| Metric | Expected Range |
|--------|---------------|
| FID Score (after finetuning) | 15-35 (good), <15 (excellent) |
| Baseline Accuracy | 92-96% |
| Best Synthetic Boost | +1-4% accuracy |
| Most Impact | Limited real data scenarios |

### Results Table Template

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    EXPERIMENT RESULTS SUMMARY                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Experiment         â”‚ Accuracy      â”‚ F1-Score â”‚ AUC    â”‚ Î” Baseline     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Baseline           â”‚ 94.2% Â± 0.3%  â”‚  0.943   â”‚ 0.967  â”‚      -         â•‘
â•‘ Real + 25% Synth   â”‚ 94.8% Â± 0.4%  â”‚  0.948   â”‚ 0.971  â”‚  +0.6%         â•‘
â•‘ Real + 50% Synth   â”‚ 95.3% Â± 0.3%  â”‚  0.952   â”‚ 0.974  â”‚  +1.1%*        â•‘
â•‘ Real + 100% Synth  â”‚ 95.7% Â± 0.2%  â”‚  0.956   â”‚ 0.978  â”‚  +1.5%*        â•‘
â•‘ 10% Real + Synth   â”‚ 91.2% Â± 0.5%  â”‚  0.908   â”‚ 0.945  â”‚  +6.1%**â€       â•‘
â•‘ 25% Real + Synth   â”‚ 93.8% Â± 0.4%  â”‚  0.936   â”‚ 0.962  â”‚  +2.4%*â€        â•‘
â•‘ Synthetic Only     â”‚ 87.3% Â± 0.6%  â”‚  0.871   â”‚ 0.912  â”‚  -6.9%         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
* p < 0.05 vs baseline | ** p < 0.01 vs baseline | â€  vs limited real baseline
```

### Key Graphs for Poster

1. **Real vs Synthetic Comparison** â€” Side-by-side image grid
2. **FID Score** â€” Bar chart (before/after finetuning)
3. **Accuracy by Configuration** â€” Bar chart with error bars
4. **Training Curves** â€” Loss over epochs
5. **ROC Curves** â€” Compare AUC across experiments

### Talking Points

1. **Why This Matters**: Medical data scarcity, privacy concerns
2. **Technical Innovation**: Full DreamBooth with text encoder training
3. **Key Finding**: Synthetic data most valuable when real data is limited
4. **Limitation**: Trained on pediatric X-rays only

---

## 14. Resources & References

### Documentation
- [Hugging Face Diffusers](https://huggingface.co/docs/diffusers)
- [DreamBooth Training](https://huggingface.co/docs/diffusers/training/dreambooth)
- [PyTorch Image Classification](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)

### Papers
- RoentGen (Stanford): [stanfordmimi.github.io/RoentGen](https://stanfordmimi.github.io/RoentGen/)
- DreamBooth: [arxiv.org/abs/2208.12242](https://arxiv.org/abs/2208.12242)
- Diffusion Models in Medical Imaging: [github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging](https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging)

### Dataset
- [hf-vision/chest-xray-pneumonia](https://huggingface.co/datasets/hf-vision/chest-xray-pneumonia)

---

## Quick Start Checklist

**On Colab (A100 80GB):**
- [ ] Setup environment
- [ ] Download and preprocess dataset
- [ ] Train Normal class (~20 min)
- [ ] Train Pneumonia class (~20 min)
- [ ] Generate 1200 synthetic images per class (~1 hr)
- [ ] Calculate FID scores
- [ ] Download synthetic images

**On Local PC (RTX 5070TI):**
- [ ] Setup conda environment
- [ ] Transfer synthetic images from Colab
- [ ] Run all 7 classifier experiments (~2-3 hrs)
- [ ] Generate statistical analysis
- [ ] Create visualizations

**Total Time: ~4-5 hours of compute**

---

**Good luck with your science fair project! ğŸ”¬ğŸ†**
